""" < - introduction to main module click to get a an overview
🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁

                █▄░█ ▄▀█ ▀█▀ █░█ █▀█ ▄▀█ █░░   █░░ ▄▀█ █▄░█ █▀▀ █░█ ▄▀█ █▀▀ █▀▀   ▀█▀ █▀█ █▀█ █░░   █▄▀ █ ▀█▀
                █░▀█ █▀█ ░█░ █▄█ █▀▄ █▀█ █▄▄   █▄▄ █▀█ █░▀█ █▄█ █▄█ █▀█ █▄█ ██▄   ░█░ █▄█ █▄█ █▄▄   █░█ █ ░█░

                                                t̴͓͍̽̀̒r̴̡͖͊̈́͠a̴͔͕̻͝͝n̸͓͕̺̿̔́s̴͇̟͎̈́̾̈́ĺ̴̢͍̼̈́͝a̸͙̠͓͛͊͝t̸͙͔͕͛̚͘e̸̺̼͆͛̔ i̸̪̪̓̔͝t̴͇̟̞͊͌̚ f̸̫̙̘̔̕͝o̴̫͕̦͛͘r̴̢͙͛͋ m̸̻̈́̒̒͜e̸̪͖̼͛̈́̓

                                                ⼕ㄖᗪ🝗 ⻏丫 Ꮆ🝗𝓝🝗丂讠丂Ꮆ讠尺
                                                        ⣤⣶⣶⣶⣶⣶⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⢿⣿⣿⡿⣿⣿⣿⣿⣿⣿⣿⣿⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⣿⡇⣿⣷⣿⣿⣿⣿⣿⣿⣯⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡰⣿⣿⣿⣇⣿⣀⠸⡟⢹⣿⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⢡⣿⣿⣿⡇⠝⠋⠀⠀⠀⢿⢿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢸⠸⣿⣿⣇⠀⠀⠀⠀⠀⠀⠊⣽⣿⣿⣿⠁⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣷⣄⠀⠀⠀⢠⣴⣿⣿⣿⠋⣠⡏⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠾⣿⣟⡻⠉⠀⠀⠀⠈⢿⠋⣿⡿⠚⠋⠁⡁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣶⣾⣿⣿⡄⠀⣳⡶⡦⡀⣿⣿⣷⣶⣤⡾⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⡆⠀⡇⡿⠉⣺⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣯⠽⢲⠇⠣⠐⠚⢻⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⡐⣾⡏⣷⠀⠀⣼⣷⡧⣿⣿⣦⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣻⣿⣿⣿⣿⣿⣮⠳⣿⣇⢈⣿⠟⣬⣿⣿⣿⣿⣿⡦⢄⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⢄⣾⣿⣿⣿⣿⣿⣿⣿⣷⣜⢿⣼⢏⣾⣿⣿⣿⢻⣿⣝⣿⣦⡑⢄⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⣠⣶⣷⣿⣿⠃⠘⣿⣿⣿⣿⣿⣿⣿⡷⣥⣿⣿⣿⣿⣿⠀⠹⣿⣿⣿⣷⡀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⣇⣤⣾⣿⣿⡿⠻⡏⠀⠀⠸⣿⣿⣿⣿⣿⣿⣮⣾⣿⣿⣿⣿⡇⠀⠀⠙⣿⣿⡿⡇⠀⠀⠀⠀⠀
                                    ⠀⠀⢀⡴⣫⣿⣿⣿⠋⠀⠀⡇⠀⠀⢰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⢘⣿⣿⣟⢦⡸⠀⠀⠀
                                    ⠀⡰⠋⣴⣿⣟⣿⠃⠀⠀⠀⠈⠀⠀⣸⣿⣿⣿⣿⣿⣿⣇⣽⣿⣿⣿⣿⣇⠀⠀⠀⠁⠸⣿⢻⣦⠉⢆⠀⠀
                                    ⢠⠇⡔⣿⠏⠏⠙⠆⠀⠀⠀⠀⢀⣜⣛⡻⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⡀⠀⠀⠀⠀⡇⡇⠹⣷⡈⡄⠀
                                    ⠀⡸⣴⡏⠀⠀⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣻⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⡇⡇⠀⢻⡿⡇⠀
                                    ⠀⣿⣿⣆⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⣰⠿⠤⠒⡛⢹⣿⠄
                                    ⠀⣿⣷⡆⠁⠀⠀⠀⠀⢠⣿⣿⠟⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠻⢷⡀⠀⠀⠀⠀⠀⣸⣿⠀
                                    ⠀⠈⠿⢿⣄⠀⠀⠀⢞⠌⡹⠁⠀⠀⢻⡇⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⢳⠀⠀⠁⠀⠀⠀⠀⢠⣿⡏⠀
                                    ⠀⠀⠀⠈⠂⠀⠀⠀⠈⣿⠁⠀⠀⠀⡇⠁⠀⠘⢿⣿⣿⠿⠟⠋⠛⠛⠛⠀⢸⠀⠀⡀⠂⠀⠀⠐⠛⠉⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠕⣠⡄⣰⡇⠀⠀⠀⢸⣧⠀⠀⠀⠀⠀⠀⠀⢀⣸⠠⡪⠊⠀⠀⠀⠀⠀⠀⠀⠀
                                    ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢫⣽⡋⠭⠶⠮⢽⣿⣆⠀⠀⠀⠀⢠⣿⣓⣽⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                                    
                                        devoted to the code and excel into the software

NLTK is a library that allows you to compile natrual language within python modules to translate , tokenize, parse, stem , tag the data
you will be translating into machine code!

🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁 🅶🅴🅽🅴🆂🅸🆂🅶🅸🆁
"""

# import NLTK
import nltk
from typing import Type

# TODO when running NLTK for the first time you must run the following code to install the dependant packages!
#nltk.download()

# important methods and shit
def openfile() -> Type[None]:
    
    # docstring
    ''' openfile() method
    open a designated file provided by user or the programmer
    '''
    
    # global
    global sentencedata
    
    # file path to text file
    file_path = r'C:\Users\daint\Documents\GitHub\Scrap-Yard\Lab-Sketches-May-2023\NaturalLanguageToolKitLabs\data\data.txt'

    # open file containing data
    with open(file=file_path, mode='r') as f:
        
        # store data within var
        sentencedata = f.read()

# TODO create a method that can handle all ways to tokenize input
def more_efficient(token_type: str) -> Type[None]:
    
    # docstring
    ''' # more_efficient(token_type: str) method
    handles every type of algorithm of tokenizers in one cohesive function that takes one parameter 'token_type' which is of
    the data type of a str.
    '''
    pass

# tokenizes words
def otaku_ahrii_word_tokenizer(user_data: str = None) -> Type[str]:
    
    """ < - otaku_ahrii_word_tokenizer() info click to open! 
:･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    
            █▀█ ▀█▀ ▄▀█ █▄▀ █░█ ▄▀█ █▀█ █░█ █ █   █░█░█ █▀█ █▀█ █▀▄    ▀█▀ █▀█ █▄▀ █▀▀ █▄░█ █ ▀█ █▀▀ █▀█ ▄▀ ▀▄
            █▄█ ░█░ █▀█ █░█ █▄█ █▀█ █▀▄ █▀█ █ █    ▀▄▀▄▀ █▄█ █▀▄ █▄▀   █░ █▄█ █░█ ██▄ █░▀█ █ █▄ ██▄ █▀▄ ▀▄ ▄▀

                                        ⣿⣿⣿⣿⣿⣿⠟⠋⠁⣀⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣿⣿
                                        ⣿⣿⣿⣿⠋⠁⠀⠀⠺⠿⢿⣿⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⣿
                                        ⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣤⣤⠀⠀⠀⠀⠀⣤⣦⣄⠀⠀
                                        ⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣶⣿⠏⣿⣿⣿⣿⣿⣁⠀⠀⠀⠛⠙⠛⠋⠀⠀
                                        ⡿⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⣰⣿⣿⣿⣿⡄⠘⣿⣿⣿⣿⣷⠄⠀⠀⠀⠀⠀⠀⠀⠀
                                        ⡇⠀⠀⠀⠀⠀⠀⠀⠸⠇⣼⣿⣿⣿⣿⣿⣷⣄⠘⢿⣿⣿⣿⣅⠀⠀⠀⠀⠀⠀⠀⠀
                                        ⠁⠀⠀⠀⣴⣿⠀⣐⣣⣸⣿⣿⣿⣿⣿⠟⠛⠛⠀⠌⠻⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀
                                        ⠀⠀⠀⣶⣮⣽⣰⣿⡿⢿⣿⣿⣿⣿⣿⡀⢿⣤⠄⢠⣄⢹⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀
        s͛ys͛ᴛⷮeͤmͫaͣᴛⷮiͥcͨ rͬeͤaͣs͛oͦniͥng            ⠀⠀⠀⣿⣿⣿⣿⣿⡘⣿⣿⣿⣿⣿⣿⠿⣶⣶⣾⣿⣿⡆⢻⣿⣿⠃⢠⠖⠛⣛⣷⠀            YASSSSSSSSS WORDSSSSSS
            🇲​​​​​🇦​​​​​🇨​​​​​🇭​​​​​🇮​​​​​🇳​​​​​🇪​​​​​ 🇨​​​​​🇴​​​​​🇩​​​​​🇪​​​​​                    ⣿⣿⣿⣿⣿⣿⣾⣿⣿⣿⣿⣿⣿⣮⣝⡻⠿⠿⢃⣄⣭⡟⢀⡎⣰⡶⣪⣿⠀
                                        ⠀⠀⠘⣿⣿⣿⠟⣛⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣿⣿⣿⡿⢁⣾⣿⢿⣿⣿⠏⠀
                                        ⠀⠀⠀⣻⣿⡟⠘⠿⠿⠎⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣵⣿⣿⠧⣷⠟⠁⠀⠀
                                        ⡇⠀⠀⢹⣿⡧⠀⡀⠀⣀⠀⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠋⢰⣿⠀⠀⠀⠀
                                        ⡇⠀⠀⠀⢻⢰⣿⣶⣿⡿⠿⢂⣿⣿⣿⣿⣿⣿⣿⢿⣻⣿⣿⣿⡏⠀⠀⠁⠀⠀⠀⠀        𝖈𝖔𝖒𝖕𝖎𝖑𝖊, 𝖈𝖔𝖒𝖕𝖎𝖑𝖊, 01010101010101
                                        ⣷⠀⠀⠀⠀⠈⠿⠟⣁⣴⣾⣿⣿⠿⠿⣛⣋⣥⣶⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ ⣿⡀
    
    🇬​​​​​🇪​​​​​🇳​​​​​🇪​​​​​🇸​​​​​🇮​​​​​🇸​​​​​ 🇸​​​​​🇾​​​​​🇳​​​​​🇴​​​​​🇵​​​​​🇸​​​​​🇮​​​​​🇸​​​​​
    The otaku_ahrii_tokenizer() is a function built to tokenize words into tokens and is a feature within the module that needs to be fed
    a positional argument in order for it to work! This can be easy as scripting a value into the corrosponding function call and send it to the
    parmeter 

    :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    """
    
    # import nltk
    import nltk
    
    # tokenize this thing
    SEXY_TOKENS = nltk.word_tokenize(sentencedata)

    # display sentence tokens
    print('The data you have provided has been tokenized via using the nltk module, the data senetnces now have their own strings:')
    return print(SEXY_TOKENS, '\n')

# tokenizes sentences
def otaku_ahrii_sentence_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_sentence_tokenizer() info click to open! 
:･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    
        █▀█ ▀█▀ ▄▀█ █▄▀ █░█ ▄▀█ █▀█ █░█ █ █  █▀ █▀▀ █▄░█ ▀█▀ █▀▀ █▄░█ █▀▀ █▀▀  ▀█▀ █▀█ █▄▀ █▀▀ █▄░█ █ ▀█ █▀▀ █▀█ ▄▀ ▀▄
        █▄█ ░█░ █▀█ █░█ █▄█ █▀█ █▀▄ █▀█ █ █  ▄█ ██▄ █░▀█ ░█░ ██▄ █░▀█ █▄▄ ██▄░  █░ █▄█ █░█ ██▄ █░▀█ █ █▄ ██▄ █▀▄ ▀▄ ▄▀





                                                    ⼕ㄖᗪ🝗 ⻏丫 Ꮆ🝗𝓝🝗丂讠丂Ꮆ讠尺
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀⠀⠀⠀⠠⠤⠶⠞⢻⣿⡿⣿⣿⣿⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠁⠀⢀⣠⣤⣤⣴⣶⣄⠀⢸⣿⠇⠻⣿⣿⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⠋⠀⠀⠰⠛⠛⠛⠻⠿⠿⣿⡇⠈⠉⠀⠀⠈⠻⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣤⣄⡀⢹⣿
                                        ⣿⣿⣿⣿⣿⣿⢿⢏⡜⢱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠾⢿⣿⠻⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⡿⢸⡞⣠⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠀⠀⠹⡍
                                        ⣉⠙⠻⣿⣹⡇⡞⢰⡟⠀⣠⠤⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹
                                        ⠈⡹⣶⢆⣿⢱⡇⢸⡷⠋⣠⣤⡈⢇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠄⢠⡀⠀⠀⠀⠈
                                        ⠀⣇⣿⢸⣿⠸⣷⠀⢧⣾⠋⠈⠻⣾⣦⠀⠀⠀⠀⠀⣴⠋⢀⣦⠀⢿⠀⠀⠀⢀
                                        ⡀⠈⣿⠘⢿⠄⠈⢀⠸⡏⠀⠀⢰⡇⡜⠀⠀⠀⠀⠀⠁⠀⠈⢸⠈⠀⠀⠀⠀⡼
                                        ⣿⣷⣿⠀⠀⠀⠀⡌⠀⢧⣀⡴⠛⢁⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡇
                                        ⣿⣿⣿⡇⠀⠀⠰⢰⠀⠀⠙⠃⢀⡾⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⡇
                                        ⣿⣿⣿⣷⠀⠀⠀⡸⠀⠀⠀⣠⣿⣿⣶⣤⣤⣀⡀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⡇
                                        ⣿⣿⣿⠏⠀⠀⢀⡇⢀⣠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⡞⠋⢸⣿⣿⣿⣿⡇
                                        ⣿⡿⠃⠀⠐⠶⣿⡿⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣞⢻⣿⣿⣿⣿⡇

    
    🇬​​​​​🇪​​​​​🇳​​​​​🇪​​​​​🇸​​​​​🇮​​​​​🇸​​​​​ 🇸​​​​​🇾​​​​​🇳​​​​​🇴​​​​​🇵​​​​​🇸​​​​​🇮​​​​​🇸​​​​​
    The otaku_ahrii_sentence_tokenizer() is a function built to tokenize sentences into tokens

    :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    """
    # import module
    import nltk
    
    # tokenizing stage
    SEXY_TOKENS = nltk.sent_tokenize(sentencedata)
    
    # display sentence tokens stage (optional)
    print('The data you have provided has been tokenized via using the nltk module, the data senetnces now have their own strings:')
    print('%s sentences found within the text file!' %(len(SEXY_TOKENS)))
    print('tokens found:')
    
    # define func()
    def iter():
        
        # iterate through tokens
        for token in SEXY_TOKENS:
            
            print(SEXY_TOKENS[token]) # display token found 
    
    # call func()
    return iter()

# TODO add different artsyle to it
# tokenizes whitespace
def otaku_ahrii_whitepace_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    
        █▀█ ▀█▀ ▄▀█ █▄▀ █░█ ▄▀█ █▀█ █░█ █ █   █░█░█ █░█ █ ▀█▀ █▀▀ █▀ █▀█ ▄▀█ █▀▀ █▀▀   ▀█▀ █▀█ █▄▀ █▀▀ █▄░█ █ ▀█ █▀▀ █▀█ ▄▀ ▀▄
        █▄█ ░█░ █▀█ █░█ █▄█ █▀█ █▀▄ █▀█ █ █   ▀▄▀▄▀ █▀█ █ ░█░ ██▄ ▄█ █▀▀ █▀█ █▄▄ ██▄    █░ █▄█ █░█ ██▄ █░▀█ █ █▄ ██▄ █▀▄ ▀▄ ▄▀

                                        
                                                    ⼕ㄖᗪ🝗 ⻏丫 Ꮆ🝗𝓝🝗丂讠丂Ꮆ讠尺
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀⠀⠀⠀⠠⠤⠶⠞⢻⣿⡿⣿⣿⣿⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠁⠀⢀⣠⣤⣤⣴⣶⣄⠀⢸⣿⠇⠻⣿⣿⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⣿⠋⠀⠀⠰⠛⠛⠛⠻⠿⠿⣿⡇⠈⠉⠀⠀⠈⠻⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣤⣄⡀⢹⣿
                                        ⣿⣿⣿⣿⣿⣿⢿⢏⡜⢱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠾⢿⣿⠻⣿⣿⣿
                                        ⣿⣿⣿⣿⣿⡿⢸⡞⣠⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠀⠀⠹⡍
                                        ⣉⠙⠻⣿⣹⡇⡞⢰⡟⠀⣠⠤⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹
                                        ⠈⡹⣶⢆⣿⢱⡇⢸⡷⠋⣠⣤⡈⢇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠄⢠⡀⠀⠀⠀⠈
                                        ⠀⣇⣿⢸⣿⠸⣷⠀⢧⣾⠋⠈⠻⣾⣦⠀⠀⠀⠀⠀⣴⠋⢀⣦⠀⢿⠀⠀⠀⢀
                                        ⡀⠈⣿⠘⢿⠄⠈⢀⠸⡏⠀⠀⢰⡇⡜⠀⠀⠀⠀⠀⠁⠀⠈⢸⠈⠀⠀⠀⠀⡼
                                        ⣿⣷⣿⠀⠀⠀⠀⡌⠀⢧⣀⡴⠛⢁⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡇    Catch those pesky words and tokeneize them even when spaces, 
                                        ⣿⣿⣿⡇⠀⠀⠰⢰⠀⠀⠙⠃⢀⡾⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⡇    tabs, newlines occur!
                                        ⣿⣿⣿⣷⠀⠀⠀⡸⠀⠀⠀⣠⣿⣿⣶⣤⣤⣀⡀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⡇
                                        ⣿⣿⣿⠏⠀⠀⢀⡇⢀⣠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⡞⠋⢸⣿⣿⣿⣿⡇ did somebody say whitespaces?. . . sounds empty to me!
                                        ⣿⡿⠃⠀⠐⠶⣿⡿⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣞⢻⣿⣿⣿⣿⡇      [         ] < - a bunch of whitespace!
    🇬​​​​​🇪​​​​​🇳​​​​​🇪​​​​​🇸​​​​​🇮​​​​​🇸​​​​​ 🇸​​​​​🇾​​​​​🇳​​​​​🇴​​​​​🇵​​​​​🇸​​​​​🇮​​​​​🇸​​​​​
    The otaku_ahrii_whitespace_tokenizer() is a function built to tokenize whitespaces into tokens found within the input given to the method
    whitespace_tokenize from the nltk module, this can be used to find whitespace and the final result is to create a list as the final output with
    all the tokens inside of that array. the difference between just tokenizing it normally is that with the 'nltk.whitespace_tokenize().tokenize()'
    the method will toekenize words on spaces, tabs, newlines!

    :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    """
    # import module
    import nltk
    
    # tokenizing stage
    tokens = nltk.WhitespaceTokenizer().tokenize(sentencedata)
    
    # display tokens
    # display sentence tokens stage (optional)
    print('The data you have provided has been tokenized via using the nltk module:')
    print('%s whitespaces found within the text file!' %(len(tokens)))
    return print('tokens found: %s' %(tokens)) # final return value of method/func()

# tokenizes tweets
def otaku_ahrii_tweet_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    
            █▀█ ▀█▀ ▄▀█ █▄▀ █░█ ▄▀█ █▀█ █░█ █ █   ▀█▀ █░█░█ █▀▀ █▀▀ ▀█▀   ▀█▀ █▀█ █▄▀ █▀▀ █▄░█ █ ▀█ █▀▀ █▀█ ▄▀ ▀▄
            █▄█ ░█░ █▀█ █░█ █▄█ █▀█ █▀▄ █▀█ █ █   ░█░ ▀▄▀▄▀ ██▄ ██▄ ░█░    █░ █▄█ █░█ ██▄ █░▀█ █ █▄ ██▄ █▀▄ ▀▄ ▄▀


                                                        ⼕ㄖᗪ🝗 ⻏丫 Ꮆ🝗𝓝🝗丂讠丂Ꮆ讠尺
                                            ⡆⣐⢕⢕⢕⢕⢕⢕⢕⢕⠅⢗⢕⢕⢕⢕⢕⢕⢕⠕⠕⢕⢕⢕⢕⢕⢕⢕⢕⢕
                                            ⢐⢕⢕⢕⢕⢕⣕⢕⢕⠕⠁⢕⢕⢕⢕⢕⢕⢕⢕⠅⡄⢕⢕⢕⢕⢕⢕⢕⢕⢕
                                            ⢕⢕⢕⢕⢕⠅⢗⢕⠕⣠⠄⣗⢕⢕⠕⢕⢕⢕⠕⢠⣿⠐⢕⢕⢕⠑⢕⢕⠵⢕
                                            ⢕⢕⢕⢕⠁⢜⠕⢁⣴⣿⡇⢓⢕⢵⢐⢕⢕⠕⢁⣾⢿⣧⠑⢕⢕⠄⢑⢕⠅⢕
                                            ⢕⢕⠵⢁⠔⢁⣤⣤⣶⣶⣶⡐⣕⢽⠐⢕⠕⣡⣾⣶⣶⣶⣤⡁⢓⢕⠄⢑⢅⢑
                                            ⠍⣧⠄⣶⣾⣿⣿⣿⣿⣿⣿⣷⣔⢕⢄⢡⣾⣿⣿⣿⣿⣿⣿⣿⣦⡑⢕⢤⠱⢐
                                            ⢠⢕⠅⣾⣿⠋⢿⣿⣿⣿⠉⣿⣿⣷⣦⣶⣽⣿⣿⠈⣿⣿⣿⣿⠏⢹⣷⣷⡅⢐
                                            ⣔⢕⢥⢻⣿⡀⠈⠛⠛⠁⢠⣿⣿⣿⣿⣿⣿⣿⣿⡀⠈⠛⠛⠁⠄⣼⣿⣿⡇⢔
                                            ⢕⢕⢽⢸⢟⢟⢖⢖⢤⣶⡟⢻⣿⡿⠻⣿⣿⡟⢀⣿⣦⢤⢤⢔⢞⢿⢿⣿⠁⢕
                                            ⢕⢕⠅⣐⢕⢕⢕⢕⢕⣿⣿⡄⠛⢀⣦⠈⠛⢁⣼⣿⢗⢕⢕⢕⢕⢕⢕⡏⣘⢕  Tokenize tweets!
                                            ⢕⢕⠅⢓⣕⣕⣕⣕⣵⣿⣿⣿⣾⣿⣿⣿⣿⣿⣿⣿⣷⣕⢕⢕⢕⢕⡵⢀⢕⢕
                                            ⢑⢕⠃⡈⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢃⢕⢕⢕      tweet tweet!
                                            ⣆⢕⠄⢱⣄⠛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⢁⢕⢕⠕⢁
                                            ⣿⣦⡀⣿⣿⣷⣶⣬⣍⣛⣛⣛⡛⠿⠿⠿⠛⠛⢛⣛⣉⣭⣤⣂⢜⠕⢑⣡⣴⣿          elon musk would be proud!

    🇬​​​​​🇪​​​​​🇳​​​​​🇪​​​​​🇸​​​​​🇮​​​​​🇸​​​​​ 🇸​​​​​🇾​​​​​🇳​​​​​🇴​​​​​🇵​​​​​🇸​​​​​🇮​​​​​🇸​​​​​
    the nltk.tweetTokenizer will search for tweets from the given input and return them as token values within a list. you may also set arguements to the 
    tweetTokenizer for matching an array of conditions such as phone numbers, remove the twitter handles from the given input and the preserve case
    postional argument that indicates wheter to preserve the capitalization of a given input from tweets.
    
    :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    """
    # import module
    import nltk
    from nltk.tokenize import TweetTokenizer
    
    # create tokenizer stage
    twitter_tokenizer = TweetTokenizer()
    
    # create tokens
    tokens = nltk.TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False, match_phone_numbers=True).tokenize(sentencedata)
    
    # display tokens
    # display sentence tokens stage (optional)
    print('The data on tweets you have provided have been tokenized via using the nltk module (nltk.TweetTokenizer().tokenize(sentencedata)):')
    print('%s tweets found within the text file!' %(len(tokens)))
    return print('token tweets found: %s' %(tokens)) # final return value of method/func()

def otaku_ahrii_lemmatizer() -> Type[str]:
        
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    
            █▀█ ▀█▀ ▄▀█ █▄▀ █░█ ▄▀█ █▀█ █░█ █ █   █░█░█ █▀█ █▀█ █▀▄   █░░ █▀▀ █▀▄▀█ █▀▄▀█ ▄▀█ ▀█▀ █ ▀█ █▀▀ █▀█
            █▄█ ░█░ █▀█ █░█ █▄█ █▀█ █▀▄ █▀█ █ █   ▀▄▀▄▀ █▄█ █▀▄ █▄▀   █▄▄ ██▄ █░▀░█ █░▀░█ █▀█ ░█░ █ █▄ ██▄ █▀▄

                                                        ⼕ㄖᗪ🝗 ⻏丫 Ꮆ🝗𝓝🝗丂讠丂Ꮆ讠尺
                                            ⣿⣿⣿⣿⣿⣿⠟⠋⠁⣀⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣿⣿
                                            ⣿⣿⣿⣿⠋⠁⠀⠀⠺⠿⢿⣿⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⣿
                                            ⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣤⣤⠀⠀⠀⠀⠀⣤⣦⣄⠀⠀
                                            ⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣶⣿⠏⣿⣿⣿⣿⣿⣁⠀⠀⠀⠛⠙⠛⠋⠀⠀
                                            ⡿⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⣰⣿⣿⣿⣿⡄⠘⣿⣿⣿⣿⣷⠄⠀⠀⠀⠀⠀⠀⠀⠀
                                            ⡇⠀⠀⠀⠀⠀⠀⠀⠸⠇⣼⣿⣿⣿⣿⣿⣷⣄⠘⢿⣿⣿⣿⣅⠀⠀⠀⠀⠀⠀⠀⠀    
                                            ⠁⠀⠀⠀⣴⣿⠀⣐⣣⣸⣿⣿⣿⣿⣿⠟⠛⠛⠀⠌⠻⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀
                                            ⠀⠀⠀⣶⣮⣽⣰⣿⡿⢿⣿⣿⣿⣿⣿⡀⢿⣤⠄⢠⣄⢹⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀            
                                            ⠀⠀⠀⣿⣿⣿⣿⣿⡘⣿⣿⣿⣿⣿⣿⠿⣶⣶⣾⣿⣿⡆⢻⣿⣿⠃⢠⠖⠛⣛⣷⠀
                                            ⠀⠀⢸⣿⣿⣿⣿⣿⣿⣾⣿⣿⣿⣿⣿⣿⣮⣝⡻⠿⠿⢃⣄⣭⡟⢀⡎⣰⡶⣪⣿⠀        ʟᴇᴍᴍᴀᴛɪᴢᴀᴛɪᴏɴ ɪs ᴜsᴇᴅ ɪɴ sᴇᴀʀᴄʜ ᴇɴɢɪɴᴇs
                                            ⠀⠀⠘⣿⣿⣿⠟⣛⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣿⣿⣿⡿⢁⣾⣿⢿⣿⣿⠏⠀                𝚛𝚎𝚍𝚞𝚌𝚎 𝚊𝚗𝚍 𝚏𝚒𝚗𝚍 𝚛𝚘𝚘𝚝 𝚕𝚎𝚖𝚖𝚊   
                                            ⠀⠀⠀⣻⣿⡟⠘⠿⠿⠎⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣵⣿⣿⠧⣷⠟⠁⠀⠀            
                                            ⡇⠀⠀⢹⣿⡧⠀⡀⠀⣀⠀⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠋⢰⣿⠀⠀⠀⠀                 Dogs is an inflection word of the word dogs!
                                            ⡇⠀⠀⠀⢻⢰⣿⣶⣿⡿⠿⢂⣿⣿⣿⣿⣿⣿⣿⢿⣻⣿⣿⣿⡏⠀⠀⠁⠀⠀⠀⠀            
                                            ⣷⠀⠀⠀⠀⠈⠿⠟⣁⣴⣾⣿⣿⠿⠿⣛⣋⣥⣶⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ ⣿⡀              reduce words down to it's lemma purest form!
                                                ₐᵣₜᵢ𝆑ᵢ𝄴ᵢₐₗ ᵢₙₜₑₗₗᵢgₑₙ𝄴ₑ ᵤₛₑₛ ᵢₜ ₐₛ wₑₗₗ
    

    🇬​​​​​🇪​​​​​🇳​​​​​🇪​​​​​🇸​​​​​🇮​​​​​🇸​​​​​ 🇸​​​​​🇾​​​​​🇳​​​​​🇴​​​​​🇵​​​​​🇸​​​​​🇮​​​​​🇸​​​​​
    lemmatization allows you to reduce the inflection of words in an input , sentence, article or whatever contextual data you pass, it 
    is very widely used in CHAT AI and serach engines to find a better search result for the root word.
    
    :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    """ 
    
    # import module
    import nltk, sys
    from nltk.stem.wordnet import WordNetLemmatizer
    global response

    # create lemmatizer
    lemmatzier = WordNetLemmatizer()
    
    # user input conditions
    while True:
        def lemmatize():
            
            # lemmatize users word
            response =  input('Select a word you would like to return the lemma for of >')
            print()
            lemma =  lemmatzier.lemmatize(word=response)

            # flow
            if response.isalpha(): # return lemma if input is str
            
                # display lemmatized input
                print('The lemma of the word %s is %s! \n' %(response, lemma))

                # re-prompt
                while True:
                    r = input('would you like to lemmatize another word? [Y/n]')
                    print()
                    
                    # re-prompt logic
                    if r == 'Y':
                        lemmatize() # recycle the function
                    elif r == 'n':
                        break # escape the loop    
                    else:
                        print('please enter a valid selection [Y/n] \n')
                        continue
    
            else:
                print('enter a word and try again! \n')
        lemmatize()
        break # escape final iterator

    # return value of method
    return None

# TODO experiement with regex tokenizers more
def otaku_ahrii_regex_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    
            █▀█ ▀█▀ ▄▀█ █▄▀ █░█ ▄▀█ █▀█ █░█ █ █   █▀█ █▀▀ █▀▀ █▀▀ ▀▄▀   ▀█▀ █▀█ █▄▀ █▀▀ █▄░█ █ ▀█ █▀▀ █▀█
            █▄█ ░█░ █▀█ █░█ █▄█ █▀█ █▀▄ █▀█ █ █   █▀▄ ██▄ █▄█ ██▄ █░█   ░█░ █▄█ █░█ ██▄ █░▀█ █ █▄ ██▄ █▀▄

                                                        ⼕ㄖᗪ🝗 ⻏丫 Ꮆ🝗𝓝🝗丂讠丂Ꮆ讠尺
                                            ⣿⣿⣿⣿⣿⣿⠟⠋⠁⣀⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣿⣿
                                            ⣿⣿⣿⣿⠋⠁⠀⠀⠺⠿⢿⣿⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⣿
                                            ⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣤⣤⣤⣤⠀⠀⠀⠀⠀⣤⣦⣄⠀⠀
                                            ⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣶⣿⠏⣿⣿⣿⣿⣿⣁⠀⠀⠀⠛⠙⠛⠋⠀⠀
                                            ⡿⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⣰⣿⣿⣿⣿⡄⠘⣿⣿⣿⣿⣷⠄⠀⠀⠀⠀⠀⠀⠀⠀
                                            ⡇⠀⠀⠀⠀⠀⠀⠀⠸⠇⣼⣿⣿⣿⣿⣿⣷⣄⠘⢿⣿⣿⣿⣅⠀⠀⠀⠀⠀⠀⠀⠀    
                                            ⠁⠀⠀⠀⣴⣿⠀⣐⣣⣸⣿⣿⣿⣿⣿⠟⠛⠛⠀⠌⠻⣿⣿⣿⡄⠀⠀⠀⠀⠀⠀⠀
                                            ⠀⠀⠀⣶⣮⣽⣰⣿⡿⢿⣿⣿⣿⣿⣿⡀⢿⣤⠄⢠⣄⢹⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀            
                                            ⠀⠀⠀⣿⣿⣿⣿⣿⡘⣿⣿⣿⣿⣿⣿⠿⣶⣶⣾⣿⣿⡆⢻⣿⣿⠃⢠⠖⠛⣛⣷⠀
                                            ⠀⠀⢸⣿⣿⣿⣿⣿⣿⣾⣿⣿⣿⣿⣿⣿⣮⣝⡻⠿⠿⢃⣄⣭⡟⢀⡎⣰⡶⣪⣿⠀   
                                            ⠀⠀⠘⣿⣿⣿⠟⣛⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣿⣿⣿⡿⢁⣾⣿⢿⣿⣿⠏⠀                  
                                            ⠀⠀⠀⣻⣿⡟⠘⠿⠿⠎⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣵⣿⣿⠧⣷⠟⠁⠀⠀            
                                            ⡇⠀⠀⢹⣿⡧⠀⡀⠀⣀⠀⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠋⢰⣿⠀⠀⠀⠀             
                                            ⡇⠀⠀⠀⢻⢰⣿⣶⣿⡿⠿⢂⣿⣿⣿⣿⣿⣿⣿⢿⣻⣿⣿⣿⡏⠀⠀⠁⠀⠀⠀⠀         
                                            ⣷⠀⠀⠀⠀⠈⠿⠟⣁⣴⣾⣿⣿⠿⠿⣛⣋⣥⣶⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ ⣿⡀           
                                                ₐᵣₜᵢ𝆑ᵢ𝄴ᵢₐₗ ᵢₙₜₑₗₗᵢgₑₙ𝄴ₑ ᵤₛₑₛ ᵢₜ ₐₛ wₑₗₗ

    🇬​​​​​🇪​​​​​🇳​​​​​🇪​​​​​🇸​​​​​🇮​​​​​🇸​​​​​ 🇸​​​​​🇾​​​​​🇳​​​​​🇴​​​​​🇵​​​​​🇸​​​​​🇮​​​​​🇸​​​​​
    find regex patterns and return them as tokens within a array.
    
    :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.: :･ﾟ✧:･.☽˚｡ ･ﾟ✧:･.:
    """ 
    
    # import nltk
    import nltk
    from nltk.tokenize import RegexpTokenizer
    
    # pattern
    gg_pattern = r'genesisgir'
    
    # display tokens
    tokens = nltk.regexp_tokenize(text= sentencedata, pattern=gg_pattern)
    return print(tokens)

def otaku_ahrii_line_tokenizer() -> Type[str]:
    
    import nltk
    tokens = nltk.line_tokenize(text=sentencedata)
    return print(tokens)

# function call list
openfile()

# TODO create more tokenizers
# tokenize words, sentences etc.
#otaku_ahrii_word_tokenizer() 
#otaku_ahrii_sentence_tokenizer()
#otaku_ahrii_whitepace_tokenizer()
#otaku_ahrii_tweet_tokenizer()
#otaku_ahrii_lemmatizer()
#otaku_ahrii_regex_tokenizer()
#otaku_ahrii_line_tokenizer()